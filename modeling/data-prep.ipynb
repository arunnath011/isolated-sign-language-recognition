{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da5bc16-c974-469d-900a-3466788a1f87",
   "metadata": {},
   "source": [
    "### Notes for Modification\n",
    "\n",
    "- Wake - Hands overlapping with face.\n",
    "- Goose - Multiple variations of signs to represent the same thing.\n",
    "- Nap - Hands overlapping with face/ Eyes close\n",
    "- Give - Hands are closed.\n",
    "- After - Hands overlapping\n",
    "- Mouth - Hand closed, hand over mouth.\n",
    "\n",
    "Commons Things:\n",
    "- Closed hands\n",
    "- Hands over face\n",
    "- Overlapping landmarks\n",
    "\n",
    "Solutions:\n",
    "- Include eye landmarks.\n",
    "- Backfill missing data with previous landmark data.\n",
    "- Preserve relative distances between landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a3ae15-2917-4b25-8720-7a06c9b2529b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 22:40:44.408766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-08 22:40:45.987538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/tensorflow/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.12.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit \n",
    "\n",
    "from layers.PreprocessLayer import PreprocessLayer\n",
    "from utils.Utils import print_shape_dtype, pd_read_s3_parquet, upload_file \n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy\n",
    "import boto3\n",
    "import io\n",
    "import wandb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "775d7831-7725-4929-8ec3-635d3fe06ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f1ed6d7-4baf-4f6e-9727-59e277149865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"./config.json\") as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac357599-d129-4fb9-bb2d-0f1bf5c8c966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\n",
    "    \"s3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65527ced-f65e-4159-abc7-cca17f2bce6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_S3_BUCKET = \"w251-asl-data\"\n",
    "TRAIN_CSV_FILE = \"raw-data/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d092a1-8ca7-4cce-9989-fb904cd60c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_file = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=TRAIN_CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef0f9e50-16f8-4391-b3b5-acbac321d24b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_SAMPLES: 94477\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(train_file.get(\"Body\"))\n",
    "\n",
    "N_SAMPLES = len(train)\n",
    "print(f'N_SAMPLES: {N_SAMPLES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66223025-c989-49dd-b662-a487c27d9bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get complete file path to file\n",
    "def get_file_path(path):\n",
    "    return f'{AWS_S3_BUCKET}/raw-data/{path}'\n",
    "\n",
    "train['file_path'] = train['path'].apply(get_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e70a14b-8be1-496c-a1ad-88aeb640d814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add ordinally Encoded Sign (assign number to each sign name)\n",
    "train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "# Dictionaries to translate sign <-> ordinal encoded sign\n",
    "SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48531589-2a82-43d6-8e54-4adcff751c41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(train.head(30))\n",
    "display(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b319f3f4-dac7-43aa-8972-ba5bbbf7b475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/competitions/asl-signs/overview/evaluation\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "#w251-asl-data/raw-data/train_landmark_files/28656/3311214787.parquet\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y']\n",
    "    data = pd_read_s3_parquet(pq_path[14:], AWS_S3_BUCKET, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b35150b8-20d6-48b4-a6d3-6cab6f1357a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HAND_IDXS: 42, N_COLS: 227\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    face: 0:468\n",
    "    left_hand: 468:489\n",
    "    pose: 489:522\n",
    "    right_hand: 522:544\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "# USE_TYPES = ['left_hand', 'pose', 'right_hand']\n",
    "# START_IDX = 468\n",
    "# LIPS_IDXS0 = np.array([\n",
    "#         61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "#         291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "#         78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "#         95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "#     ])\n",
    "# # Landmark indices in original data\n",
    "# LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "# RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "# POSE_IDXS0 = np.arange(502, 512)\n",
    "# LANDMARK_IDXS0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, POSE_IDXS0))\n",
    "\n",
    "LIPS_IDXS0 = [0, 11, 12, 13, 14, 15, 17, 37, 38, 39, 40, 41, 42, 61, 62, 72, 73, \n",
    "        74, 76, 77, 78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 146, \n",
    "        178, 179, 180, 181, 183, 184, 185, 191, 267, 268, 269, 270, 271, 272, \n",
    "        291, 292, 302, 303, 304, 306, 307, 308, 310, 311, 312, 314, 316, 317, \n",
    "        318, 319, 320, 321, 324, 325, 375, 402, 403, 404, 405, 407, 408, 409, 415]\n",
    "\n",
    "EYES_IDXS0 = [  6,   7,  22,  23,  24,  25,  26,  30,  31,  33,  56, 110, 112,\n",
    "       113, 122, 128, 130, 133, 144, 145, 153, 154, 155, 157, 158, 159,\n",
    "       160, 161, 163, 168, 173, 188, 189, 190, 193, 196, 197, 232, 233,\n",
    "       243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 259, 260,\n",
    "       263, 286, 339, 341, 351, 357, 359, 362, 373, 374, 380, 381, 382,\n",
    "       384, 385, 386, 387, 388, 390, 398, 412, 413, 414, 417, 419, 453,\n",
    "       463, 464, 465, 466, 467]\n",
    "\n",
    "POSE_IDXS0 = np.arange(489, 514)\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "\n",
    "LANDMARK_IDXS0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, EYES_IDXS0, POSE_IDXS0))\n",
    "N_COLS = LANDMARK_IDXS0.size\n",
    "\n",
    "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
    "\n",
    "# Landmark indices in processed data\n",
    "# LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LIPS_IDXS0)).squeeze()\n",
    "# LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LEFT_HAND_IDXS0)).squeeze()\n",
    "# RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, RIGHT_HAND_IDXS0)).squeeze()\n",
    "# HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, HAND_IDXS0)).squeeze()\n",
    "# POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, POSE_IDXS0)).squeeze()\n",
    "\n",
    "print(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd859631-8867-48f8-8e79-73b9d907ae7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIPS_START: 0, LEFT_HAND_START: 77, RIGHT_HAND_START: 98, POSE_START: 119\n"
     ]
    }
   ],
   "source": [
    "LIPS_START = 0\n",
    "LEFT_HAND_START = LIPS_IDXS.size\n",
    "RIGHT_HAND_START = LEFT_HAND_START + LEFT_HAND_IDXS.size\n",
    "POSE_START = RIGHT_HAND_START + RIGHT_HAND_IDXS.size\n",
    "\n",
    "print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c319d84-7646-4a42-b4ce-8e2e1c9d3f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess_layer = PreprocessLayer(config[\"N_ROWS\"], config[\"N_DIMS\"], HAND_IDXS0, LANDMARK_IDXS0, config[\"INPUT_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aae5f2ac-9674-4aac-9fb3-bc623a4ce9df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    face: 0:468\n",
    "    left_hand: 468:489\n",
    "    pose: 489:522\n",
    "    right_hand: 522:544\n",
    "        \n",
    "\"\"\"\n",
    "def get_data(file_path):\n",
    "    # Load Raw Data\n",
    "    data = load_relevant_data_subset(file_path)\n",
    "    # Process Data Using Tensorflow\n",
    "    data = preprocess_layer(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2a897-3852-4f3b-9823-88be2b3977ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = config[\"DATA_VERSION\"]\n",
    "\n",
    "# Get the full dataset\n",
    "def get_x_y():\n",
    "    # Create arrays to save data\n",
    "    X = np.zeros([N_SAMPLES, INPUT_SIZE, N_COLS, N_DIMS], dtype=np.float32)\n",
    "    y = np.zeros([N_SAMPLES], dtype=np.int32)\n",
    "    NON_EMPTY_FRAME_IDXS = np.full([N_SAMPLES, INPUT_SIZE], -1, dtype=np.float32)\n",
    "\n",
    "    for row_idx, (file_path, sign_ord) in enumerate(tqdm(train[['file_path', 'sign_ord']].values)):\n",
    "        if row_idx % 5000 == 0:\n",
    "            print(f'Generated {row_idx}/{N_SAMPLES}')\n",
    "\n",
    "        data, non_empty_frame_idxs = get_data(file_path)\n",
    "        X[row_idx] = data\n",
    "        y[row_idx] = sign_ord\n",
    "        NON_EMPTY_FRAME_IDXS[row_idx] = non_empty_frame_idxs\n",
    "        if np.isnan(data).sum() > 0:\n",
    "            print(row_idx)\n",
    "            return data\n",
    "    \n",
    "    # Save X/y\n",
    "    np.save('X.npy', X)\n",
    "    np.save('y.npy', y)\n",
    "    np.save('NON_EMPTY_FRAME_IDXS.npy', NON_EMPTY_FRAME_IDXS)\n",
    "    \n",
    "    # Put to S3\n",
    "    upload_file(\"./X.npy\", AWS_S3_BUCKET, f'processed-data/v{version}/X.npy')\n",
    "    upload_file(\"./y.npy\", AWS_S3_BUCKET, f'processed-data/v{version}/y.npy')\n",
    "    upload_file(\"./NON_EMPTY_FRAME_IDXS.npy\", AWS_S3_BUCKET, f'processed-data/v{version}/NON_EMPTY_FRAME_IDXS.npy')\n",
    "    \n",
    "    return X, y, NON_EMPTY_FRAME_IDXS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4a87a-890c-4985-9840-d45ca9e91569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PREPROCESS_DATA:\n",
    "    X, y, NON_EMPTY_FRAME_IDXS = get_x_y()\n",
    "else:\n",
    "    X = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/X.npy')\n",
    "    X = np.load(io.BytesIO(X['Body'].read()))\n",
    "    \n",
    "    y = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/y.npy')\n",
    "    y = np.load(io.BytesIO(y['Body'].read()))\n",
    "    \n",
    "    NON_EMPTY_FRAME_IDXS = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/NON_EMPTY_FRAME_IDXS.npy')\n",
    "    NON_EMPTY_FRAME_IDXS = np.load(io.BytesIO(NON_EMPTY_FRAME_IDXS['Body'].read()))\n",
    "    \n",
    "print_shape_dtype([X, y, NON_EMPTY_FRAME_IDXS], ['X', 'y', 'NON_EMPTY_FRAME_IDXS'])\n",
    "print(f'# NaN Values X: {np.isnan(X).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492118d3-97a3-4a2d-a2a3-1d2eb8485cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(pd.Series(y).value_counts().to_frame('Class Count').iloc[[0,1,2,3,4, -5,-4,-3,-2,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864425d-7123-42bb-b1f1-5a082e86e6dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/X.npy')\n",
    "X = np.load(io.BytesIO(X['Body'].read()))\n",
    "\n",
    "y = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/y.npy')\n",
    "y = np.load(io.BytesIO(y['Body'].read()))\n",
    "\n",
    "NON_EMPTY_FRAME_IDXS = s3_client.get_object(Bucket=AWS_S3_BUCKET, Key=f'processed-data/v{version}/NON_EMPTY_FRAME_IDXS.npy')\n",
    "NON_EMPTY_FRAME_IDXS = np.load(io.BytesIO(NON_EMPTY_FRAME_IDXS['Body'].read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e894d-d064-4644-a8e2-6f5098ec3333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp = PreprocessLayerMod(config[\"N_ROWS\"], config[\"N_DIMS\"], HAND_IDXS0, LANDMARK_IDXS0, config[\"INPUT_SIZE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3faa0b8-ff23-4272-88d2-1edfbad930ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execution when there are less frames than specificed input size. \n",
    "data0 = load_relevant_data_subset(\"w251-asl-data/raw-data/train_landmark_files/28656/1000106739.parquet\")\n",
    "N_FRAMES0 = tf.shape(data0)[0] # 11\n",
    "gather = tf.gather(data0, HAND_IDXS0, axis=1) # Returns all frames with hand landmark data\n",
    "frames_hands_nansum = tf.experimental.numpy.nanmean(gather, axis=[1,2]) # Get mean across X,Y sum them together\n",
    "non_empty_frames_idxs = tf.where(frames_hands_nansum > 0) \n",
    "non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1) # Get indicies where there are no NAN\n",
    "data = tf.gather(data0, non_empty_frames_idxs, axis=0) # Put frames that have non-empty indicies into single tensor\n",
    "non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32) # Cast\n",
    "N_FRAMES = tf.shape(data)[0] # Number of frames in the video with non-empty hands\n",
    "data = tf.gather(data, LANDMARK_IDXS0, axis=1) # Filters out landmarks that have not been selected\n",
    "# Pads to the right of non_empty_frames_idxs with -1's\n",
    "non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, config[\"INPUT_SIZE\"]-N_FRAMES]], constant_values=-1)\n",
    "data = tf.pad(data, [[0, config[\"INPUT_SIZE\"]-N_FRAMES], [0,0], [0,0]], constant_values=0) # Fill all the -1's with zero values.\n",
    "data = tf.where(tf.math.is_nan(data), 0.0, data) # Fill all NANs with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2c708521-f9b0-4c43-a019-63f5d9434225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LIPS_IDXS0 = [0, 11, 12, 13, 14, 15, 17, 37, 38, 39, 40, 41, 42, 61, 62, 72, 73, \n",
    "#         74, 76, 77, 78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 146, \n",
    "#         178, 179, 180, 181, 183, 184, 185, 191, 267, 268, 269, 270, 271, 272, \n",
    "#         291, 292, 302, 303, 304, 306, 307, 308, 310, 311, 312, 314, 316, 317, \n",
    "#         318, 319, 320, 321, 324, 325, 375, 402, 403, 404, 405, 407, 408, 409, 415]\n",
    "\n",
    "# EYES_IDXS0 = [  6,   7,  22,  23,  24,  25,  26,  30,  31,  33,  56, 110, 112,\n",
    "#        113, 122, 128, 130, 133, 144, 145, 153, 154, 155, 157, 158, 159,\n",
    "#        160, 161, 163, 168, 173, 188, 189, 190, 193, 196, 197, 232, 233,\n",
    "#        243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 259, 260,\n",
    "#        263, 286, 339, 341, 351, 357, 359, 362, 373, 374, 380, 381, 382,\n",
    "#        384, 385, 386, 387, 388, 390, 398, 412, 413, 414, 417, 419, 453,\n",
    "#        463, 464, 465, 466, 467]\n",
    "\n",
    "FACE_IDXS0 = [0, 6, 7, 11, 12, 13, 14, 15, 17, 22, 23, 24, 25, 26, 30, 31, 33, 37, 38, 39, 40, 41, 42, 56, \n",
    "             61, 62, 72, 73, 74, 76, 77, 78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 110, 112, 113, \n",
    "             122, 128, 130, 133, 144, 145, 146, 153, 154, 155, 157, 158, 159, 160, 161, 163, 168, 173, 178, \n",
    "             179, 180, 181, 183, 184, 185, 188, 189, 190, 191, 193, 196, 197, 232, 233, 243, 244, 245, 246, \n",
    "             247, 249, 252, 253, 254, 255, 256, 259, 260, 263, 267, 268, 269, 270, 271, 272, 286, 291, 292, \n",
    "             302, 303, 304, 306, 307, 308, 310, 311, 312, 314, 316, 317, 318, 319, 320, 321, 324, 325, 339, \n",
    "             341, 351, 357, 359, 362, 373, 374, 375, 380, 381, 382, 384, 385, 386, 387, 388, 390, 398, 402, \n",
    "             403, 404, 405, 407, 408, 409, 412, 413, 414, 415, 417, 419, 453, 463, 464, 465, 466, 467]\n",
    "\n",
    "\n",
    "POSE_IDXS0 = np.arange(489, 514)\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "\n",
    "LANDMARK_IDXS0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, EYES_IDXS0, POSE_IDXS0))\n",
    "N_COLS = LANDMARK_IDXS0.size\n",
    "\n",
    "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
    "\n",
    "# Landmark indices in processed data\n",
    "# LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LIPS_IDXS0)).squeeze()\n",
    "# LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LEFT_HAND_IDXS0)).squeeze()\n",
    "# RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, RIGHT_HAND_IDXS0)).squeeze()\n",
    "# HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, HAND_IDXS0)).squeeze()\n",
    "# POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, POSE_IDXS0)).squeeze()\n",
    "\n",
    "FACE_START = 0\n",
    "LEFT_HAND_START = len(FACE_IDXS0)\n",
    "POSE_START = LEFT_HAND_START + LEFT_HAND_IDXS0.size\n",
    "RIGHT_HAND_START = POSE_START + POSE_IDXS0.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dc287499-ce32-473f-8aa5-dd3e14ef00df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 11, 160, 2), dtype=float32, numpy=\n",
       "array([[[[0.3109382 , 0.30274147],\n",
       "         [0.3108517 , 0.30263487],\n",
       "         [0.31070736, 0.3025791 ],\n",
       "         ...,\n",
       "         [0.31003988, 0.29980758],\n",
       "         [0.31016448, 0.29940003],\n",
       "         [0.3101387 , 0.29969093]],\n",
       "\n",
       "        [[0.30949372, 0.3010725 ],\n",
       "         [0.30935383, 0.30120784],\n",
       "         [0.3091575 , 0.30136055],\n",
       "         ...,\n",
       "         [0.3087556 , 0.29856545],\n",
       "         [0.3087838 , 0.2983931 ],\n",
       "         [0.30884987, 0.29858443]],\n",
       "\n",
       "        [[0.30696207, 0.301286  ],\n",
       "         [0.30691883, 0.3012452 ],\n",
       "         [0.30694762, 0.30109647],\n",
       "         ...,\n",
       "         [0.30833372, 0.29799432],\n",
       "         [0.3085047 , 0.2975407 ],\n",
       "         [0.3084519 , 0.29800206]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.29300418, 0.30127618],\n",
       "         [0.29308486, 0.30110818],\n",
       "         [0.2932472 , 0.30086678],\n",
       "         ...,\n",
       "         [0.2930791 , 0.30263004],\n",
       "         [0.2929298 , 0.30289853],\n",
       "         [0.2930756 , 0.30265015]],\n",
       "\n",
       "        [[0.29334256, 0.30017957],\n",
       "         [0.2934748 , 0.3001314 ],\n",
       "         [0.29358357, 0.30009434],\n",
       "         ...,\n",
       "         [0.29289815, 0.30227786],\n",
       "         [0.2929288 , 0.30237693],\n",
       "         [0.2928583 , 0.30222696]],\n",
       "\n",
       "        [[0.29203036, 0.29962438],\n",
       "         [0.29207364, 0.29967043],\n",
       "         [0.2921309 , 0.29975763],\n",
       "         ...,\n",
       "         [0.29205525, 0.30233428],\n",
       "         [0.29189077, 0.3024647 ],\n",
       "         [0.2919133 , 0.30233064]]],\n",
       "\n",
       "\n",
       "       [[[0.30300584, 0.30168945],\n",
       "         [0.3029923 , 0.30167514],\n",
       "         [0.30296892, 0.30166808],\n",
       "         ...,\n",
       "         [0.3027894 , 0.30129954],\n",
       "         [0.30280823, 0.3012531 ],\n",
       "         [0.3028185 , 0.30128783]],\n",
       "\n",
       "        [[0.30277854, 0.30144805],\n",
       "         [0.30275655, 0.30146733],\n",
       "         [0.30272502, 0.30148944],\n",
       "         ...,\n",
       "         [0.3025984 , 0.3011446 ],\n",
       "         [0.30260286, 0.30112952],\n",
       "         [0.3026247 , 0.30115154]],\n",
       "\n",
       "        [[0.3023801 , 0.30147892],\n",
       "         [0.30237326, 0.30147278],\n",
       "         [0.30237722, 0.3014507 ],\n",
       "         ...,\n",
       "         [0.30253562, 0.30107334],\n",
       "         [0.3025613 , 0.30102488],\n",
       "         [0.30256486, 0.3010798 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.30018356, 0.30147752],\n",
       "         [0.30019575, 0.30145282],\n",
       "         [0.30022112, 0.30141702],\n",
       "         ...,\n",
       "         [0.30026662, 0.30165163],\n",
       "         [0.30024457, 0.30168247],\n",
       "         [0.30025265, 0.30165237]],\n",
       "\n",
       "        [[0.30023682, 0.30131888],\n",
       "         [0.30025712, 0.30131054],\n",
       "         [0.30027404, 0.30130377],\n",
       "         ...,\n",
       "         [0.30023974, 0.3016077 ],\n",
       "         [0.30024442, 0.30161846],\n",
       "         [0.30021995, 0.30160025]],\n",
       "\n",
       "        [[0.3000303 , 0.3012386 ],\n",
       "         [0.30003658, 0.30124342],\n",
       "         [0.3000454 , 0.30125442],\n",
       "         ...,\n",
       "         [0.30011436, 0.30161473],\n",
       "         [0.30009001, 0.30162922],\n",
       "         [0.30007786, 0.301613  ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Modified\n",
    "data0 = load_relevant_data_subset(\"w251-asl-data/raw-data/train_landmark_files/28656/1000106739.parquet\")\n",
    "data0 = tf.stack([data0, tf.math.add(data0, 3)], 0)\n",
    "\n",
    "N_FRAMES = tf.shape(data0)[1]\n",
    "data = tf.gather(data0, LANDMARK_IDXS0, axis=2)\n",
    "\n",
    "# TODO: Add batch dimension when slicing. \n",
    "x = data\n",
    "\n",
    "face = tf.slice(x, [0, 0, FACE_START, 0], [-1, N_FRAMES, LEFT_HAND_START, 2])\n",
    "face = tf.keras.utils.normalize(face, axis=1, order=2)\n",
    "\n",
    "# left_hand = tf.slice(x, [0, 0, LEFT_HAND_START, 0], [-1, N_FRAMES, POSE_START, 2])\n",
    "# left_hand = tf.keras.utils.normalize(left_hand, axis=1, order=2)\n",
    "\n",
    "# pose = tf.slice(x, [0, 0, POSE_START, 0], [-1, N_FRAMES, RIGHT_HAND_START, 2])\n",
    "# pose = tf.keras.utils.normalize(pose, axis=1, order=2)\n",
    "\n",
    "# right_hand = tf.slice(x, [0, 0, RIGHT_HAND_START, 0], [-1, N_FRAMES, tf.shape(x)[1], 2])\n",
    "# right_hand = tf.keras.utils.normalize(right_hand, axis=1, order=2)\n",
    "\n",
    "\n",
    "# data = tf.keras.utils.normalize(\n",
    "#     data0[0], axis=-1, order=2)\n",
    "\n",
    "# # LIPS\n",
    "# lips = tf.slice(x, [0,0,LIPS_START,0], [-1,config[\"INPUT_SIZE\"], 40, 2])\n",
    "# lips = tf.where(\n",
    "#         tf.math.equal(lips, 0.0),\n",
    "#         0.0,\n",
    "#         (lips - LIPS_MEAN) / LIPS_STD,\n",
    "#     )\n",
    "# lips = tf.reshape(lips, [-1, config[\"INPUT_SIZE\"], 40*2])\n",
    "# # LEFT HAND\n",
    "# left_hand = tf.slice(x, [0,0,40,0], [-1,config[\"INPUT_SIZE\"], 21, 2])\n",
    "# left_hand = tf.where(\n",
    "#         tf.math.equal(left_hand, 0.0),\n",
    "#         0.0,\n",
    "#         (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "#     )\n",
    "# left_hand = tf.reshape(left_hand, [-1, config[\"INPUT_SIZE\"], 21*2])\n",
    "# # RIGHT HAND\n",
    "# right_hand = tf.slice(x, [0,0,61,0], [-1,config[\"INPUT_SIZE\"], 21, 2])\n",
    "# right_hand = tf.where(\n",
    "#         tf.math.equal(right_hand, 0.0),\n",
    "#         0.0,\n",
    "#         (right_hand - RIGHT_HANDS_MEAN) / RIGHT_HANDS_STD,\n",
    "#     )\n",
    "# right_hand = tf.reshape(right_hand, [-1, config[\"INPUT_SIZE\"], 21*2])\n",
    "# # POSE\n",
    "# pose = tf.slice(x, [0,0,82,0], [-1,config[\"INPUT_SIZE\"], 10, 2])\n",
    "# pose = tf.where(\n",
    "#         tf.math.equal(pose, 0.0),\n",
    "#         0.0,\n",
    "#         (pose - POSE_MEAN) / POSE_STD,\n",
    "#     )\n",
    "# pose = tf.reshape(pose, [-1, config[\"INPUT_SIZE\"], 10*2])\n",
    "\n",
    "# x = lips, left_hand, right_hand, pose\n",
    "\n",
    "\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dee0217-dbaa-4d4b-9610-8f991a3fe5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if N_FRAMES < 4: # Number of frames we want\n",
    "    non_empty_frames_idxs = tf.pad(tf.range(0, N_FRAMES, 1), [[0, 32-N_FRAMES]], constant_values=-1)\n",
    "    data = tf.pad(data, [[0, 32-N_FRAMES], [0,0], [0,0]], constant_values=-1)\n",
    "    # Fill NaN Values With 0\n",
    "    data = tf.where(tf.math.is_nan(data), -1., data)\n",
    "    # return data, non_empty_frames_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e6aa2-c729-467d-9edd-9a1e2a14f68c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessLayerMod(tf.keras.layers.Layer):\n",
    "    def __init__(self, N_ROWS, N_DIMS, HAND_IDXS0, LANDMARK_IDXS0, INPUT_SIZE):\n",
    "        super(PreprocessLayerMod, self).__init__()\n",
    "        self.N_ROWS = N_ROWS\n",
    "        self.N_DIMS = N_DIMS\n",
    "        self.HAND_IDXS0 = HAND_IDXS0\n",
    "        self.LANDMARK_IDXS0 = LANDMARK_IDXS0\n",
    "        self.INPUT_SIZE = INPUT_SIZE\n",
    "        self.N_COLS = LANDMARK_IDXS0.size\n",
    "        \n",
    "    def pad_edge(self, t, repeats, side):\n",
    "        if side == 'LEFT':\n",
    "            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n",
    "        elif side == 'RIGHT':\n",
    "            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n",
    "    \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, 543, 2], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, data0):\n",
    "        # Number of Frames in Video\n",
    "        N_FRAMES0 = tf.shape(data0)[0]\n",
    "        \n",
    "        # Filter Out Frames With Empty Hand Data\n",
    "        frames_hands_nansum = tf.experimental.numpy.nanmean(tf.gather(data0, self.HAND_IDXS0, axis=1), axis=[1,2])\n",
    "        non_empty_frames_idxs = tf.where(frames_hands_nansum > 0)\n",
    "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
    "        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n",
    "        \n",
    "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
    "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32) \n",
    "\n",
    "        # Number of Frames in Filtered Video\n",
    "        N_FRAMES = tf.shape(data)[0]\n",
    "        \n",
    "        # Gather Relevant Landmark Columns\n",
    "        data = tf.gather(data, self.LANDMARK_IDXS0, axis=1)\n",
    "        \n",
    "        # Video fits in self.INPUT_SIZE\n",
    "        if N_FRAMES < self.INPUT_SIZE:\n",
    "            # Pad With -1 to indicate padding\n",
    "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, self.INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
    "            # Pad Data With Zeros\n",
    "            data = tf.pad(data, [[0, self.INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            return data, non_empty_frames_idxs\n",
    "        # Video needs to be downsampled to INPUT_SIZE\n",
    "        else:\n",
    "            # Repeat\n",
    "            if N_FRAMES < self.INPUT_SIZE**2:\n",
    "                repeats = tf.math.floordiv(self.INPUT_SIZE * self.INPUT_SIZE, N_FRAMES0)\n",
    "                data = tf.repeat(data, repeats=repeats, axis=0)\n",
    "                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n",
    "\n",
    "            # Pad To Multiple Of Input Size\n",
    "            pool_size = tf.math.floordiv(len(data), self.INPUT_SIZE)\n",
    "            if tf.math.mod(len(data), self.INPUT_SIZE) > 0:\n",
    "                pool_size += 1\n",
    "\n",
    "            if pool_size == 1:\n",
    "                pad_size = (pool_size * self.INPUT_SIZE) - len(data)\n",
    "            else:\n",
    "                pad_size = (pool_size * self.INPUT_SIZE) % len(data)\n",
    "\n",
    "            # Pad Start/End with Start/End value\n",
    "            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(self.INPUT_SIZE, 2)\n",
    "            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(self.INPUT_SIZE, 2)\n",
    "            if tf.math.mod(pad_size, 2) > 0:\n",
    "                pad_right += 1\n",
    "\n",
    "            # Pad By Concatenating Left/Right Edge Values\n",
    "            data = self.pad_edge(data, pad_left, 'LEFT')\n",
    "            data = self.pad_edge(data, pad_right, 'RIGHT')\n",
    "\n",
    "            # Pad Non Empty Frame Indices\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n",
    "\n",
    "            # Reshape to Mean Pool\n",
    "            data = tf.reshape(data, [self.INPUT_SIZE, -1, self.N_COLS, self.N_DIMS])\n",
    "            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [self.INPUT_SIZE, -1])\n",
    "\n",
    "            # Mean Pool\n",
    "            data = tf.experimental.numpy.nanmean(data, axis=1)\n",
    "            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n",
    "\n",
    "            # Fill NaN Values With 0\n",
    "            data = tf.where(tf.math.is_nan(data), 0.0, data)\n",
    "            \n",
    "            return data, non_empty_frames_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcb222-ae5e-4b62-9cb6-91536f625230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIPS_IDXS0 = [0, 11, 12, 13, 14, 15, 17, 37, 38, 39, 40, 41, 42, 61, 62, 72, 73, \n",
    "        74, 76, 77, 78, 80, 81, 82, 84, 86, 87, 88, 89, 90, 91, 95, 96, 146, \n",
    "        178, 179, 180, 181, 183, 184, 185, 191, 267, 268, 269, 270, 271, 272, \n",
    "        291, 292, 302, 303, 304, 306, 307, 308, 310, 311, 312, 314, 316, 317, \n",
    "        318, 319, 320, 321, 324, 325, 375, 402, 403, 404, 405, 407, 408, 409, 415]\n",
    "\n",
    "EYES_IDXS0 = [  6,   7,  22,  23,  24,  25,  26,  30,  31,  33,  56, 110, 112,\n",
    "       113, 122, 128, 130, 133, 144, 145, 153, 154, 155, 157, 158, 159,\n",
    "       160, 161, 163, 168, 173, 188, 189, 190, 193, 196, 197, 232, 233,\n",
    "       243, 244, 245, 246, 247, 249, 252, 253, 254, 255, 256, 259, 260,\n",
    "       263, 286, 339, 341, 351, 357, 359, 362, 373, 374, 380, 381, 382,\n",
    "       384, 385, 386, 387, 388, 390, 398, 412, 413, 414, 417, 419, 453,\n",
    "       463, 464, 465, 466, 467]\n",
    "\n",
    "POSE_IDXS0 = np.arange(489, 514)\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489)\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543)\n",
    "\n",
    "LANDMARK_IDXS0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, EYES_IDXS0, POSE_IDXS0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3eb1fc-88f3-4ba5-be78-6d7a5b4ecb47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd_read_s3_parquet(\"raw-data/train_landmark_files/28656/1000106739.parquet\", AWS_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c95b20-e4a3-44c0-bc2b-5eb011e9a7f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[(data['frame']==29) & (data[\"type\"] == \"pose\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa148b6a-d52e-48c6-9403-2341ae477e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[(data['frame']==29) & (data.index.isin(POSE_IDXS0))].plot.scatter(x='x',y='y', marker='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
